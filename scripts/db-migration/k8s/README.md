# â˜¸ï¸ Database Migration Kubernetes Job

This directory contains Kubernetes manifests to run the database migration as a Job, eliminating the need for manual pod setup and file copying.

## ğŸ“ Contents

- ğŸ“œ `configmap-script.yaml` - ConfigMap containing the `database_migrator.sh` script
- âš™ï¸ `configmap-config.yaml` - ConfigMap containing the `db_config.yaml` configuration
- ğŸš€ `job.yaml` - Job definition (adaptable for both standard and airgapped environments)
- ğŸ”§ `kustomization.yaml` - Kustomize configuration for deployment

## ğŸ“‹ Prerequisites

### ğŸŒ For Standard Environments

- â˜¸ï¸ Kubernetes cluster with internet access
- ğŸ”§ kubectl configured and connected to your cluster
- ğŸ“¦ Access to pull images from:
  - `ghcr.io/aleph-alpha/shared-images/pharia-helper:latest`

### ğŸ”’ For Airgapped Environments

âš ï¸ **IMPORTANT**: For airgapped environments, you **must** prepare the following:

#### ğŸ› ï¸ Required: Helper Container Image

The migration script requires a container image with these tools pre-installed:
- ğŸ˜ **PostgreSQL 17.x client tools**: `psql`, `pg_dump` (version 17.x)
- ğŸ“„ **yq**: YAML processor for parsing configuration files
- ğŸš **bash**: Shell for running the migration script

The default image used is `ghcr.io/aleph-alpha/shared-images/pharia-helper:latest` which includes all required tools.

#### ğŸ“¦ Required: Images in Internal Registry

Ensure the helper image is available in your internal container registry:

| ğŸ“· Image Purpose | ğŸŒ Source Image | ğŸ¢ Required In Registry |
|---------------|--------------|---------------------|
| ğŸš€ Migration container | `ghcr.io/aleph-alpha/shared-images/pharia-helper:latest` | `your-registry.com/pharia-helper:latest` |

**ğŸ”§ Steps to prepare:**

1. Pull the image from the source registry:
```bash
docker pull ghcr.io/aleph-alpha/shared-images/pharia-helper:latest
```

2. Tag and push to your internal registry:
```bash
docker tag ghcr.io/aleph-alpha/shared-images/pharia-helper:latest \
  your-registry.company.com/pharia-helper:latest
docker push your-registry.company.com/pharia-helper:latest
```

#### âœ… Verification Checklist

Before deploying to airgapped environment:

- [ ] ğŸ“¦ Helper image available in internal registry
- [ ] ğŸ¢ Internal registry accessible from Kubernetes cluster
- [ ] ğŸ”‘ Image pull secrets configured (if required)
- [ ] ğŸŒ Network policies allow pod to connect to source and destination databases
- [ ] ğŸ” Database credentials configured in `configmap-config.yaml`
- [ ] ğŸ“ `job.yaml` updated with internal registry URL

## ğŸš€ Quick Start

### ğŸŒ For Standard Environments (with Internet Access)

#### 1. ğŸ” Update Credentials

Edit `configmap-config.yaml` and fill in the database passwords:

```sh
vim configmap-config.yaml
```

Find and replace all `password: ""` fields with actual credentials.

#### 2. ğŸš€ Deploy with Kustomize

```sh
kubectl apply -k .
```

Or apply individually:

```sh
kubectl apply -f configmap-script.yaml
kubectl apply -f configmap-config.yaml
kubectl apply -f job.yaml
```

### ğŸ”’ For Airgapped Environments (without Internet Access)

#### 1. ğŸ“¦ Ensure Helper Image is Available

Make sure the helper image is available in your internal registry (see Prerequisites section above).

#### 2. ğŸ”§ Modify job.yaml for Airgapped Use

Edit `job.yaml` to use your internal registry:

```bash
# Make a backup
cp job.yaml job.yaml.bak

# Edit the file
vim job.yaml
```

**âš ï¸ Required changes in job.yaml:**

Update the container image reference (around line 36):

```yaml
# Change from:
image: ghcr.io/aleph-alpha/shared-images/pharia-helper:latest

# To your internal registry:
image: your-registry.company.com/pharia-helper:latest
```

#### 3. Update Database Credentials

```bash
vim configmap-config.yaml
# Fill in all password fields with actual credentials
```

#### 4. Deploy

```bash
kubectl apply -f configmap-script.yaml
kubectl apply -f configmap-config.yaml
kubectl apply -f job.yaml
```

Or using Kustomize:

```bash
kubectl apply -k .
```

### 3. ğŸ‘€ Monitor Progress

```sh
# Watch job status
kubectl get job db-migration -n pharia-ai -w

# View logs
kubectl logs -f job/db-migration -n pharia-ai

# Get pod name and check status
kubectl get pods -n pharia-ai -l app=db-migration
```

### 4. ğŸ§¹ Cleanup

```sh
# Delete the job
kubectl delete job db-migration -n pharia-ai

# Delete ConfigMaps
kubectl delete configmap db-migration-script db-migration-config -n pharia-ai
```

## âš™ï¸ Job Configuration

The Job is configured with:

- â° **Timeout**: 2 hours (`activeDeadlineSeconds: 7200`)
- ğŸ”„ **Retries**: 0 attempts (`backoffLimit: 0`) - fails fast on errors
- ğŸ” **Restart Policy**: Never (`restartPolicy: Never`)
- ğŸ—“ï¸ **Retention**: Kept for 24 hours after completion (`ttlSecondsAfterFinished: 86400`)
- ğŸ’» **Resources**:
  - ğŸ“Š Requests: 256Mi memory, 250m CPU
  - ğŸš« Limits: 1Gi memory, 1000m CPU
- ğŸ’¾ **Storage**:
  - ğŸ—ƒï¸ Dumps: 50Gi ephemeral storage
  - ğŸ“ Logs: 1Gi ephemeral storage

## âœ¨ Features

### ğŸ›¡ï¸ Pre-flight Permission Checks

Before starting the actual migration, the script performs comprehensive permission checks to ensure users have the necessary privileges:

**ğŸ“– For Source Databases (Dump Operations):**
- ğŸ”— CONNECT privilege to the database
- ğŸ“Š Ability to read from `information_schema` tables
- ğŸ—ï¸ Access to schema information (required for `pg_dump`)
- ğŸ” Proper database access permissions

**âœï¸ For Destination Databases (Restore Operations):**
- ğŸ”— CONNECT privilege to the database
- ğŸ—ï¸ CREATE privilege on the database (required for creating tables)
- ğŸ“ CREATE privilege on the `public` schema (or target schemas)
- ğŸ” Proper database access permissions

If any permission issues are detected, the script will:
1. Display clear error messages indicating which permissions are missing
2. Provide SQL commands to grant the necessary permissions
3. Abort the migration before attempting any operations

**Example Permission Errors:**

```
âŒ pharia_temporal: User 'myuser' lacks CREATE privilege on destination database (required for restore)
âš ï¸  pharia_temporal: Grant CREATE permission with: GRANT CREATE ON DATABASE temporal TO myuser;
```

This pre-flight check prevents partial migrations and helps diagnose permission issues early.

## ğŸ—ï¸ Architecture

The Job uses a single container (`migrator`) that runs the migration script:

- ğŸ“¦ **Image**: `ghcr.io/aleph-alpha/shared-images/pharia-helper:latest`
- ğŸ› ï¸ **Pre-installed tools**:
  - ğŸ˜ PostgreSQL 17.x client tools (`psql`, `pg_dump`)
  - ğŸ“„ `yq` YAML processor
  - ğŸš `bash` shell
- âš™ï¸ **Configuration**: Mounted from ConfigMaps
- ğŸ’¾ **Storage**: Ephemeral volumes for dumps and logs
- ğŸš€ **Execution**: Runs `database_migrator.sh` with verbose logging

## ğŸ“‚ Volumes

| ğŸ“ Volume | ğŸ“‹ Type | ğŸ¯ Purpose | ğŸ“ Size |
|--------|------|---------|------|
| `migration-script` | configMap | The migration shell script | N/A |
| `migration-config` | configMap | Database configuration YAML | N/A |
| `dumps` | emptyDir | Temporary storage for database dumps | 50Gi |
| `logs` | emptyDir | Migration logs | 1Gi |

## ğŸ”§ Troubleshooting

### âŒ Job Failed

Check the pod logs:

```sh
# Get pod name
kubectl get pods -n pharia-ai -l app=db-migration

# View logs
kubectl logs <pod-name> -n pharia-ai

# View logs from previous run (if restarted)
kubectl logs <pod-name> -n pharia-ai --previous
```

### ğŸ”„ Restarting a Failed Job

If a job fails and you need to restart it after making configuration changes, you must delete the existing job before reapplying:

```sh
# Delete the failed job
kubectl delete job db-migration -n pharia-ai

# Reapply the job (and ConfigMaps if deleted)
kubectl apply -f configmap-script.yaml
kubectl apply -f configmap-config.yaml
kubectl apply -f job.yaml

# Or use Kustomize to reapply everything
kubectl apply -k .
```

**âš ï¸ Important Notes:**
- Kubernetes Jobs are immutable once created, so you cannot modify a running or completed job
- Always delete the job first before making changes to the configuration
- If you only need to update credentials, you can delete just the config ConfigMap and reapply it, but you'll still need to delete and recreate the job

### ğŸŒ Connection Issues

Verify that the pod can reach the database services:

```sh
# Exec into the pod
kubectl exec -it <pod-name> -n pharia-ai -- bash

# Test connection
psql -h <database-host> -p 5432 -U <username> -d <database>
```

### ğŸ” Permission Issues

If the pre-flight checks fail due to missing permissions, you'll need to grant them on the respective databases:

**ğŸ“– For Source Database (Dump) Permissions:**

```sql
-- Connect as a superuser or database owner
GRANT CONNECT ON DATABASE <database_name> TO <username>;
GRANT USAGE ON SCHEMA public TO <username>;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO <username>;
GRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO <username>;

-- If you have custom schemas, grant on them too
GRANT USAGE ON SCHEMA <schema_name> TO <username>;
GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO <username>;
```

**âœï¸ For Destination Database (Restore) Permissions:**

```sql
-- Connect as a superuser or database owner
GRANT CONNECT ON DATABASE <database_name> TO <username>;
GRANT CREATE ON DATABASE <database_name> TO <username>;
GRANT ALL PRIVILEGES ON SCHEMA public TO <username>;

-- If you have custom schemas, grant on them too
GRANT ALL PRIVILEGES ON SCHEMA <schema_name> TO <username>;
```

**ğŸ” Quick Permission Check:**

You can manually verify permissions by connecting to the database:

```sh
# Check if user can dump (source)
kubectl exec -it <pod-name> -n pharia-ai -- \
  psql -h <source-host> -U <username> -d <database> \
  -c "SELECT has_database_privilege('<username>', '<database>', 'CONNECT');"

# Check if user can restore (destination)
kubectl exec -it <pod-name> -n pharia-ai -- \
  psql -h <dest-host> -U <username> -d <database> \
  -c "SELECT has_database_privilege('<username>', '<database>', 'CREATE');"
```

### ğŸ’» Insufficient Resources

If the job fails due to resource constraints, adjust the limits in `job.yaml`:

```yaml
resources:
  requests:
    memory: "512Mi"
    cpu: "500m"
  limits:
    memory: "2Gi"
    cpu: "2000m"
```

### â° Timeout Issues

For large databases, increase the timeout:

```yaml
spec:
  activeDeadlineSeconds: 14400  # 4 hours
```

Also update the timeout in `configmap-config.yaml`:

```yaml
config:
  timeouts:
    restore: 7200  # 2 hours
    dump: 7200     # 2 hours
```

## ğŸ”’ Security Considerations

1. ğŸ‘® **RBAC**: The job uses the default service account. For production, create a dedicated service account with minimal permissions.

2. ğŸŒ **Network Policies**: Ensure the job pod has network access to both source and destination databases.

## ğŸ”¬ Advanced Usage

### ğŸ§ª Dry Run Mode

To test without actually migrating data, edit the job command in `job.yaml`:

```yaml
command:
- /bin/bash
- -c
- |
  ./database_migrator.sh --config db_config.yaml --dry-run --verbose
```

### ğŸ¯ Selective Migration

To migrate only specific databases, edit `configmap-config.yaml` and remove unwanted database entries from the `databases` list.

### ğŸ› ï¸ Custom Configuration

You can override the configuration by updating the `configmap-config.yaml` file. Common customizations:

- Adjust timeouts for large databases
- Change dump options
- Modify retry behavior
- Update dump directory location

## ğŸ“š References

- â˜¸ï¸ [Kubernetes Jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/)
- ğŸ“ [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/)
- ğŸ”§ [Kustomize](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/)
- ğŸ˜ [PostgreSQL Documentation](https://www.postgresql.org/docs/17/)
